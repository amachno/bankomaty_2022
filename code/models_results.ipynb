{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_to_remove = [79, 80, 81]\n",
    "# adding columns for which there are NaNs in SARIMA\n",
    "numbers_to_remove += [1, 4, 15, 16, 17, 18, 26, 28, 29, 40, 42, 47, 54, 59, 67, 71, 77]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from utils import ends_with_number\n",
    "\n",
    "# Define the folder where the CSV files are located\n",
    "folder_path = r'D:\\AGH\\bankomaty_2022\\data\\redatyprognoz'\n",
    "\n",
    "# Use glob to find all CSV files in the folder\n",
    "file_pattern = folder_path + r'\\prognozy_W_*.csv'\n",
    "file_list = glob.glob(file_pattern)\n",
    "\n",
    "# Create an empty list to hold the DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Loop over each file and read it into a DataFrame\n",
    "for file in file_list:\n",
    "    df = pd.read_csv(file)\n",
    "    period_id = df['date'].min()\n",
    "    df['periodID'] = period_id\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate the DataFrames into a single DataFrame\n",
    "xgb_raw_df = pd.concat(df_list)\n",
    "\n",
    "# Get a list of column names to keep\n",
    "columns_to_keep = [column for column in xgb_raw_df.columns if not ends_with_number(column, ['{:02d}'.format(num) for num in numbers_to_remove])]\n",
    "\n",
    "# Remove the columns from the data frame\n",
    "xgb_raw_df = xgb_raw_df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Define the directory where the Excel files are stored\n",
    "directory = \"D:/AGH/bankomaty_2022/data/Wielkość wypłat\"\n",
    "\n",
    "# Get a list of all Excel files in the directory\n",
    "excel_files = [f for f in os.listdir(directory) if f.endswith(\".xlsx\")]\n",
    "\n",
    "# Create an empty list to store the data frames\n",
    "data_frames = []\n",
    "\n",
    "# Loop through each Excel file and load the \"prediction errors\" sheet\n",
    "for i, excel_file in enumerate(excel_files):\n",
    "    file_path = os.path.join(directory, excel_file)\n",
    "    xl = pd.ExcelFile(file_path)\n",
    "    sheet_name = \"prediction errors\"\n",
    "    df = xl.parse(sheet_name)\n",
    "    period_id = os.path.splitext(excel_file)[0]\n",
    "    df['periodID'] = period_id\n",
    "    data_frames.append(df)\n",
    "\n",
    "# Combine all data frames into a single data frame\n",
    "sarima_raw_df = pd.concat(data_frames)\n",
    "\n",
    "# Get a list of column names to keep\n",
    "columns_to_keep = [column for column in sarima_raw_df.columns if not ends_with_number(column, ['{:02d}'.format(num) for num in numbers_to_remove])]\n",
    "\n",
    "# Remove the columns from the data frame\n",
    "sarima_raw_df = sarima_raw_df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path of the Excel file\n",
    "file_path = r'D:\\AGH\\bankomaty_2022\\data\\replaced 17-12-2022.xlsx'\n",
    "\n",
    "# Read all sheets with names ending in \"_errors\" into a list of data frames\n",
    "df_list = []\n",
    "xl = pd.ExcelFile(file_path)\n",
    "for sheet_name in xl.sheet_names:\n",
    "    if sheet_name.endswith(\"_errors\"):\n",
    "        df = xl.parse(sheet_name)\n",
    "        period_id = sheet_name[:-7]\n",
    "        day, month, year = period_id.split(\"_\")\n",
    "        new_period_id = f\"{year}_{month}_{day}\"\n",
    "        df['periodID'] = new_period_id\n",
    "        df_list.append(df)\n",
    "\n",
    "# Concatenate the data frames into a single data frame\n",
    "bayes_raw_df = pd.concat(df_list)\n",
    "\n",
    "# Rename the \"Unnamed\" column to \"name\"\n",
    "bayes_raw_df = bayes_raw_df.rename(columns={\"Unnamed: 0\": \"name\"})\n",
    "\n",
    "# Remove columns with names starting with 'ATM_N_'\n",
    "bayes_raw_df = bayes_raw_df.loc[:, ~bayes_raw_df.columns.str.startswith('ATM_N_')]\n",
    "\n",
    "# Print the resulting data frame\n",
    "bayes_raw_df\n",
    "\n",
    "# Get a list of column names to keep\n",
    "columns_to_keep = [column for column in bayes_raw_df.columns if not ends_with_number(column, ['{:02d}'.format(num) for num in numbers_to_remove])]\n",
    "\n",
    "# Remove the columns from the data frame\n",
    "bayes_raw_df = bayes_raw_df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming bayes_raw_df is your original DataFrame\n",
    "\n",
    "# Keep only rows with column name 'MAPE' or 'SMAPE'\n",
    "bayes_filtered_df = bayes_raw_df[bayes_raw_df['name'].isin(['MAPE', 'SMAPE'])]\n",
    "\n",
    "# Convert 'periodID' column from string to datetime format\n",
    "bayes_filtered_df['periodID'] = pd.to_datetime(bayes_filtered_df['periodID'], format='%Y_%m_%d')\n",
    "\n",
    "# Remove the row with '26.04.2019'\n",
    "bayes_filtered_df = bayes_filtered_df[bayes_filtered_df['periodID'] != '2019-04-26']\n",
    "\n",
    "bayes_filtered_df['model'] = 'BVAR'\n",
    "# Reset index\n",
    "bayes_filtered_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "unique_period_ids = bayes_filtered_df['periodID'].unique()\n",
    "\n",
    "# Sort the unique dates\n",
    "sorted_unique_dates = sorted(unique_period_ids)\n",
    "\n",
    "# Create a dictionary mapping the sorted unique dates to their rank\n",
    "date_rank_mapping = {date: rank + 1 for rank, date in enumerate(sorted_unique_dates)}\n",
    "\n",
    "# Apply the mapping to the 'periodID' column\n",
    "bayes_filtered_df['periodID'] = bayes_filtered_df['periodID'].map(date_rank_mapping)\n",
    "\n",
    "\n",
    "bayes_filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming xgb_raw_df is your original DataFrame\n",
    "\n",
    "# Keep only rows with column 'date' containing 'MAPE' or 'SMAPE'\n",
    "xgb_filtered_df = xgb_raw_df[xgb_raw_df['date'].isin(['MAPE', 'SMAPE'])]\n",
    "\n",
    "# Convert 'periodID' column from string to datetime format\n",
    "xgb_filtered_df['periodID'] = pd.to_datetime(xgb_filtered_df['periodID'], format='%Y-%m-%d')\n",
    "\n",
    "# Remove the rows with '26.04.2019' and '26.04.2020'\n",
    "xgb_filtered_df = xgb_filtered_df[(xgb_filtered_df['periodID'] != '2019-04-26') & (xgb_filtered_df['periodID'] != '2020-04-26')]\n",
    "\n",
    "# Reset index\n",
    "xgb_filtered_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Change column name 'date' to 'name'\n",
    "xgb_filtered_df.rename(columns={'date': 'name'}, inplace=True)\n",
    "\n",
    "# Get all column names starting with 'ATM_W_'\n",
    "atm_columns = [col for col in xgb_filtered_df.columns if col.startswith('ATM_W_')]\n",
    "xgb_filtered_df['model'] = 'XGB'\n",
    "\n",
    "# Multiply all columns with the format 'ATM_W_XX' by 100\n",
    "xgb_filtered_df[atm_columns] = xgb_filtered_df[atm_columns] * 100\n",
    "\n",
    "unique_period_ids = xgb_filtered_df['periodID'].unique()\n",
    "\n",
    "# Sort the unique dates\n",
    "sorted_unique_dates = sorted(unique_period_ids)\n",
    "\n",
    "# Create a dictionary mapping the sorted unique dates to their rank\n",
    "date_rank_mapping = {date: rank + 1 for rank, date in enumerate(sorted_unique_dates)}\n",
    "\n",
    "# Apply the mapping to the 'periodID' column\n",
    "xgb_filtered_df['periodID'] = xgb_filtered_df['periodID'].map(date_rank_mapping)\n",
    "\n",
    "xgb_filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Assuming sarima_raw_df is your original DataFrame\n",
    "\n",
    "# Keep only rows with column 'name' containing 'MAPE' or 'SMAPE'\n",
    "sarima_filtered_df = sarima_raw_df[sarima_raw_df['name'].isin(['MAPE', 'SMAPE'])]\n",
    "\n",
    "# Extract date from 'periodID' column\n",
    "sarima_filtered_df['periodID'] = sarima_filtered_df['periodID'].apply(lambda x: x[6:16])\n",
    "\n",
    "# Convert 'periodID' column from string to datetime format\n",
    "sarima_filtered_df['periodID'] = pd.to_datetime(sarima_filtered_df['periodID'], format='%Y-%m-%d')\n",
    "\n",
    "# Remove the rows with '26.04.2019' and '26.04.2020'\n",
    "sarima_filtered_df = sarima_filtered_df[(sarima_filtered_df['periodID'] != '2018-08-01') & (sarima_filtered_df['periodID'] != '2019-08-01')]\n",
    "\n",
    "sarima_filtered_df['model'] = 'SARIMA'\n",
    "# Reset index\n",
    "sarima_filtered_df.reset_index(drop=True, inplace=True)\n",
    "unique_period_ids = sarima_filtered_df['periodID'].unique()\n",
    "\n",
    "# Sort the unique dates\n",
    "sorted_unique_dates = sorted(unique_period_ids)\n",
    "\n",
    "# Create a dictionary mapping the sorted unique dates to their rank\n",
    "date_rank_mapping = {date: rank + 1 for rank, date in enumerate(sorted_unique_dates)}\n",
    "\n",
    "# Apply the mapping to the 'periodID' column\n",
    "sarima_filtered_df['periodID'] = sarima_filtered_df['periodID'].map(date_rank_mapping)\n",
    "sarima_filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_unique_dates = xgb_filtered_df['periodID'].unique()\n",
    "bayes_unique_dates = bayes_filtered_df['periodID'].unique()\n",
    "sarima_unique_dates = sarima_filtered_df['periodID'].unique()\n",
    "\n",
    "xgb_unique_dates_sorted = sorted(xgb_unique_dates)\n",
    "bayes_unique_dates_sorted = sorted(bayes_unique_dates)\n",
    "sarima_unique_dates_sorted = sorted(sarima_unique_dates)\n",
    "\n",
    "print(\"Sorted unique dates in XGB DataFrame:\", xgb_unique_dates_sorted)\n",
    "print(\"Sorted unique dates in Bayes DataFrame:\", bayes_unique_dates_sorted)\n",
    "print(\"Sorted unique dates in SARIMA DataFrame:\", sarima_unique_dates_sorted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the three DataFrames\n",
    "concatenated_df = pd.concat([xgb_filtered_df, bayes_filtered_df, sarima_filtered_df], axis=0)\n",
    "\n",
    "# Reset the index of the concatenated DataFrame\n",
    "concatenated_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Get a list of columns to drop\n",
    "columns_to_drop = [col for col in concatenated_df.columns if col.startswith('pred_W_')]\n",
    "\n",
    "# Drop the columns\n",
    "concatenated_df = concatenated_df.drop(columns_to_drop, axis=1)\n",
    "\n",
    "concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter MAPE and SMAPE rows separately\n",
    "mape_df = concatenated_df[concatenated_df['name'] == 'MAPE']\n",
    "smape_df = concatenated_df[concatenated_df['name'] == 'SMAPE']\n",
    "\n",
    "# Set the plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the bar plots for MAPE and SMAPE\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10), sharey=True)\n",
    "\n",
    "# MAPE plot\n",
    "sns.barplot(x=\"model\", y=\"value\", hue=\"ATM\", data=mape_df.melt(id_vars=[\"model\", \"name\", \"periodID\"], var_name=\"ATM\", value_name=\"value\"), ax=ax1)\n",
    "ax1.set_title(\"MAPE Comparison\")\n",
    "ax1.set_ylabel(\"MAPE\")\n",
    "ax1.set_xlabel(\"Model\")\n",
    "ax1.legend(title=\"ATM\", loc=\"upper right\", ncol=2)\n",
    "\n",
    "# SMAPE plot\n",
    "sns.barplot(x=\"model\", y=\"value\", hue=\"ATM\", data=smape_df.melt(id_vars=[\"model\", \"name\", \"periodID\"], var_name=\"ATM\", value_name=\"value\"), ax=ax2)\n",
    "ax2.set_title(\"SMAPE Comparison\")\n",
    "ax2.set_ylabel(\"SMAPE\")\n",
    "ax2.set_xlabel(\"Model\")\n",
    "ax2.legend(title=\"ATM\", loc=\"upper right\", ncol=2)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the mean MAPE and SMAPE for each model and time period\n",
    "mape_mean = mape_df.groupby(['model', 'periodID']).mean()\n",
    "smape_mean = smape_df.groupby(['model', 'periodID']).mean()\n",
    "\n",
    "# Create bar plots for mean MAPE and SMAPE\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "\n",
    "# MAPE plot\n",
    "sns.barplot(x=\"model\", y=\"value\", hue=\"periodID\", data=mape_mean.reset_index().melt(id_vars=[\"model\", \"periodID\"], value_name=\"value\"), ax=ax1)\n",
    "ax1.set_title(\"Mean MAPE Comparison\")\n",
    "ax1.set_ylabel(\"Mean MAPE\")\n",
    "ax1.set_xlabel(\"Model\")\n",
    "ax1.legend(title=\"Time Period\", loc=\"upper right\")\n",
    "\n",
    "# SMAPE plot\n",
    "sns.barplot(x=\"model\", y=\"value\", hue=\"periodID\", data=smape_mean.reset_index().melt(id_vars=[\"model\", \"periodID\"], value_name=\"value\"), ax=ax2)\n",
    "ax2.set_title(\"Mean SMAPE Comparison\")\n",
    "ax2.set_ylabel(\"Mean SMAPE\")\n",
    "ax2.set_xlabel(\"Model\")\n",
    "ax2.legend(title=\"Time Period\", loc=\"upper right\")\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(r'D:\\AGH\\bankomaty_2022\\pics\\mape_smape_comparison.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select a subset of ATMs\n",
    "atm_subset = ['ATM_W_02', 'ATM_W_03', 'ATM_W_05']\n",
    "\n",
    "# Filter the dataframe\n",
    "mape_subset = mape_df[atm_subset + ['model', 'periodID']]\n",
    "smape_subset = smape_df[atm_subset + ['model', 'periodID']]\n",
    "\n",
    "# Create bar plots for mean MAPE and SMAPE\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "\n",
    "# MAPE plot\n",
    "sns.barplot(x=\"model\", y=\"value\", hue=\"periodID\", data=mape_subset.reset_index().melt(id_vars=[\"model\", \"periodID\"], value_name=\"value\"), ax=ax1)\n",
    "ax1.set_title(\"Mean MAPE Comparison\")\n",
    "ax1.set_ylabel(\"Mean MAPE\")\n",
    "ax1.set_xlabel(\"Model\")\n",
    "ax1.legend(title=\"Time Period\", loc=\"upper left\")\n",
    "\n",
    "# SMAPE plot\n",
    "sns.barplot(x=\"model\", y=\"value\", hue=\"periodID\", data=smape_subset.reset_index().melt(id_vars=[\"model\", \"periodID\"], value_name=\"value\"), ax=ax2)\n",
    "ax2.set_title(\"Mean SMAPE Comparison\")\n",
    "ax2.set_ylabel(\"Mean SMAPE\")\n",
    "ax2.set_xlabel(\"Model\")\n",
    "ax2.legend(title=\"Time Period\", loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_nans = concatenated_df.isna().any().any()\n",
    "print(has_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of unique time periods\n",
    "time_periods = concatenated_df[\"periodID\"].unique()\n",
    "\n",
    "# Loop through the time periods and create a bar chart for each one\n",
    "for period in time_periods:\n",
    "    # Filter the data for the current time period\n",
    "    mape_period = mape_df[mape_df[\"periodID\"] == period]\n",
    "    smape_period = smape_df[smape_df[\"periodID\"] == period]\n",
    "\n",
    "    # Create bar plots for the current time period\n",
    "    # (Use the same plotting code as before, but replace 'mape_df' with 'mape_period' and 'smape_df' with 'smape_period')\n",
    "    # Filter the dataframe\n",
    "    mape_subset = smape_period[atm_subset + ['model', 'periodID']]\n",
    "    smape_subset = smape_period[atm_subset + ['model', 'periodID']]\n",
    "\n",
    "    # Create bar plots for mean MAPE and SMAPE\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "\n",
    "    # MAPE plot\n",
    "    sns.barplot(x=\"model\", y=\"value\", hue=\"periodID\", data=mape_subset.reset_index().melt(id_vars=[\"model\", \"periodID\"], value_name=\"value\"), ax=ax1)\n",
    "    ax1.set_title(\"Mean MAPE Comparison\")\n",
    "    ax1.set_ylabel(\"Mean MAPE\")\n",
    "    ax1.set_xlabel(\"Model\")\n",
    "    ax1.legend(title=\"Time Period\", loc=\"upper right\")\n",
    "\n",
    "    # SMAPE plot\n",
    "    sns.barplot(x=\"model\", y=\"value\", hue=\"periodID\", data=smape_subset.reset_index().melt(id_vars=[\"model\", \"periodID\"], value_name=\"value\"), ax=ax2)\n",
    "    ax2.set_title(\"Mean SMAPE Comparison\")\n",
    "    ax2.set_ylabel(\"Mean SMAPE\")\n",
    "    ax2.set_xlabel(\"Model\")\n",
    "    ax2.legend(title=\"Time Period\", loc=\"upper right\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean MAPE and SMAPE for each model and period across all ATMs\n",
    "mean_mape = mape_df.groupby(['model', 'periodID']).mean().stack().reset_index(name='value')\n",
    "mean_mape['metric'] = 'MAPE'\n",
    "\n",
    "mean_smape = smape_df.groupby(['model', 'periodID']).mean().stack().reset_index(name='value')\n",
    "mean_smape['metric'] = 'SMAPE'\n",
    "\n",
    "# Combine the mean MAPE and SMAPE DataFrames\n",
    "mean_metrics_df = pd.concat([mean_mape, mean_smape], ignore_index=True)\n",
    "\n",
    "# Pivot the combined dataframe for all ATMs\n",
    "all_atms_comparison_table = mean_metrics_df.pivot_table(index=['metric', 'model'], columns='periodID', values='value').round(2)\n",
    "\n",
    "# Save the table to a CSV file\n",
    "all_atms_comparison_table.to_csv(r'D:\\AGH\\bankomaty_2022\\results\\all_atms_model_comparison.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Filter only rows with 'MAPE' metric\n",
    "mape_df = concatenated_df[concatenated_df['name'] == 'MAPE']\n",
    "\n",
    "# Calculate the rank for each model based on MAPE for each ATM and period\n",
    "atm_columns = mape_df.columns[1:-2]\n",
    "mape_df_rank = mape_df.set_index(['model', 'periodID'])[atm_columns].stack().reset_index(name='MAPE').rename(columns={'level_2': 'ATM'})\n",
    "mape_df_rank['rank'] = mape_df_rank.groupby(['ATM', 'periodID'])['MAPE'].rank(ascending=True, method='dense')\n",
    "\n",
    "# Save the ranking table to a CSV file\n",
    "mape_df_rank.to_csv('D:/AGH/bankomaty_2022/results/rank_table_ATM_period.csv', index=False)\n",
    "\n",
    "# Count the number of times each model had rank 1, 2, and 3\n",
    "rank_count = mape_df_rank.groupby(['model', 'rank']).size().reset_index(name='count')\n",
    "\n",
    "# Pivot the rank count table to have model as index and rank as columns\n",
    "rank_count_pivot = rank_count.pivot_table(index='model', columns='rank', values='count').fillna(0)\n",
    "\n",
    "# Save the summary table to a CSV file\n",
    "rank_count_pivot.to_csv('D:/AGH/bankomaty_2022/results/summary_rank_count_ATM_period.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Filter only rows with 'SMAPE' metric\n",
    "smape_df = concatenated_df[concatenated_df['name'] == 'SMAPE']\n",
    "\n",
    "# Calculate the rank for each model based on SMAPE for each ATM and period\n",
    "atm_columns = smape_df.columns[1:-2]\n",
    "smape_df_rank = smape_df.set_index(['model', 'periodID'])[atm_columns].stack().reset_index(name='SMAPE').rename(columns={'level_2': 'ATM'})\n",
    "smape_df_rank['rank'] = smape_df_rank.groupby(['ATM', 'periodID'])['SMAPE'].rank(ascending=True, method='dense')\n",
    "\n",
    "# Save the ranking table to a CSV file\n",
    "smape_df_rank.to_csv('D:/AGH/bankomaty_2022/results/rank_table_ATM_period_SMAPE.csv', index=False)\n",
    "\n",
    "# Count the number of times each model had rank 1, 2, and 3\n",
    "smape_rank_count = smape_df_rank.groupby(['model', 'rank']).size().reset_index(name='count')\n",
    "\n",
    "# Pivot the rank count table to have model as index and rank as columns\n",
    "smape_rank_count_pivot = smape_rank_count.pivot_table(index='model', columns='rank', values='count').fillna(0)\n",
    "\n",
    "# Save the summary table to a CSV file\n",
    "smape_rank_count_pivot.to_csv('D:/AGH/bankomaty_2022/results/summary_rank_count_ATM_period_SMAPE.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_rank_count_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "period_pairs = [(1, 5), (2, 6), (3, 7), (4, 8)]\n",
    "models = ['BVAR', 'SARIMA', 'XGB']\n",
    "concatenated_mape = concatenated_df[concatenated_df['name'] == \"MAPE\"]\n",
    "atms = concatenated_mape.columns[1:-2]\n",
    "\n",
    "transition_matrices = {}\n",
    "\n",
    "for period_pair in period_pairs:\n",
    "    early_period, later_period = period_pair\n",
    "    \n",
    "    early_period_df = concatenated_mape[concatenated_mape['periodID'] == early_period]\n",
    "    later_period_df = concatenated_mape[concatenated_mape['periodID'] == later_period]\n",
    "    \n",
    "    transition_matrix = pd.DataFrame(index=models, columns=models, data=0)\n",
    "    \n",
    "    for atm in atms:\n",
    "        early_best_model = early_period_df.loc[early_period_df[atm].idxmin()]['model']\n",
    "        later_best_model = later_period_df.loc[later_period_df[atm].idxmin()]['model']\n",
    "        \n",
    "        transition_matrix.at[early_best_model, later_best_model] += 1\n",
    "    \n",
    "    transition_matrices[period_pair] = transition_matrix\n",
    "\n",
    "for period_pair, transition_matrix in transition_matrices.items():\n",
    "    print(f\"Transition matrix for periods {period_pair} (MAPE):\\n{transition_matrix}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "# The code for generating the transition_matrices should be placed here.\n",
    "\n",
    "# Create a new Excel workbook\n",
    "writer = pd.ExcelWriter(r'D:\\AGH\\bankomaty_2022\\results\\matrices_mape.xlsx', engine='openpyxl')\n",
    "\n",
    "# Write each transition matrix to a separate sheet in the workbook\n",
    "for period_pair, transition_matrix in transition_matrices.items():\n",
    "    sheet_name = f\"Periods {period_pair[0]}-{period_pair[1]}\"\n",
    "    transition_matrix.to_excel(writer, sheet_name=sheet_name)\n",
    "\n",
    "# Save the Excel workbook\n",
    "writer.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "period_pairs = [(1, 5), (2, 6), (3, 7), (4, 8)]\n",
    "models = ['BVAR', 'SARIMA', 'XGB']\n",
    "concatenated_smape = concatenated_df[concatenated_df['name'] == \"SMAPE\"]\n",
    "atms = concatenated_smape.columns[1:-2]\n",
    "\n",
    "transition_matrices = {}\n",
    "\n",
    "for period_pair in period_pairs:\n",
    "    early_period, later_period = period_pair\n",
    "    \n",
    "    early_period_df = concatenated_smape[concatenated_smape['periodID'] == early_period]\n",
    "    later_period_df = concatenated_smape[concatenated_smape['periodID'] == later_period]\n",
    "    \n",
    "    transition_matrix = pd.DataFrame(index=models, columns=models, data=0)\n",
    "    \n",
    "    for atm in atms:\n",
    "        early_best_model = early_period_df.loc[early_period_df[atm].idxmin()]['model']\n",
    "        later_best_model = later_period_df.loc[later_period_df[atm].idxmin()]['model']\n",
    "        \n",
    "        transition_matrix.at[early_best_model, later_best_model] += 1\n",
    "    \n",
    "    transition_matrices[period_pair] = transition_matrix\n",
    "\n",
    "for period_pair, transition_matrix in transition_matrices.items():\n",
    "    print(f\"Transition matrix for periods {period_pair} (MAPE):\\n{transition_matrix}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "# The code for generating the transition_matrices should be placed here.\n",
    "\n",
    "# Create a new Excel workbook\n",
    "writer = pd.ExcelWriter(r'D:\\AGH\\bankomaty_2022\\results\\matrices_smape.xlsx', engine='openpyxl')\n",
    "\n",
    "# Write each transition matrix to a separate sheet in the workbook\n",
    "for period_pair, transition_matrix in transition_matrices.items():\n",
    "    sheet_name = f\"Periods {period_pair[0]}-{period_pair[1]}\"\n",
    "    transition_matrix.to_excel(writer, sheet_name=sheet_name)\n",
    "\n",
    "# Save the Excel workbook\n",
    "writer.save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bankomaty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
